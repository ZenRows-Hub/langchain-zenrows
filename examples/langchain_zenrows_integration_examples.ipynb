{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain-ZenRows Integration Examples\n",
        "\n",
        "This notebook demonstrates how to use the `langchain-zenrows` package for web scraping with LangChain.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. Install the package: `pip install langchain-zenrows`\n",
        "2. Get your ZenRows API key from [ZenRows](https://app.zenrows.com/register?prod=universal_scraper)\n",
        "3. For LangChain agents, you'll also need an LLM API key (OpenAI, Anthropic, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n",
        "\n",
        "First, let's set up our API keys and import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "from langchain_zenrows import ZenRowsUniversalScraper\n",
        "\n",
        "# Set your ZenRows API key\n",
        "os.environ[\"ZENROWS_API_KEY\"] = \"<YOUR_ZENROWS_API_KEY>\"\n",
        "\n",
        "# For LangChain agents, also set your LLM API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n",
        "\n",
        "print(\"✅ Environment variables set!\")\n",
        "print(f\"ZenRows API Key: {'Set' if os.environ.get('ZENROWS_API_KEY') else 'Not set'}\")\n",
        "print(f\"OpenAI API Key: {'Set' if os.environ.get('OPENAI_API_KEY') else 'Not set'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Web Scraping\n",
        "\n",
        "Let's start with simple HTML extraction and explore different output formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the ZenRows scraper\n",
        "scraper = ZenRowsUniversalScraper()\n",
        "\n",
        "# Basic HTML scraping\n",
        "result = scraper.invoke({\"url\": \"https://httpbin.io/html\"})\n",
        "\n",
        "print(f\"Content length: {len(result)} characters\")\n",
        "print(f\"First 300 characters:\")\n",
        "print(result[:300] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get content in clean markdown format\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.example.com\", \n",
        "    \"response_type\": \"markdown\"\n",
        "})\n",
        "\n",
        "print(\"Content in Markdown format:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JavaScript Rendering and Single Page Applications\n",
        "\n",
        "Modern websites often require JavaScript rendering to display dynamic content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scrape JavaScript-rendered content with advanced parameters\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/javascript-rendering\",\n",
        "    \"js_render\": True,\n",
        "    \"wait\": 3000,  # Wait 3 seconds for content to load\n",
        "    \"wait_for\": \".product-name\",  # Wait for specific element\n",
        "    \"response_type\": \"markdown\",\n",
        "    \"premium_proxy\": True,\n",
        "    \"proxy_country\": \"us\"\n",
        "})\n",
        "\n",
        "print(\"SPA content (rendered with JavaScript):\")\n",
        "print(result[:800] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSS Data Extraction\n",
        "\n",
        "Extract specific data using CSS selectors to get structured information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract specific data using CSS selectors\n",
        "css_selector = json.dumps({\n",
        "    \"title\": \".site-title\",\n",
        "    \"product_names\": \".product-name\",\n",
        "    \"prices\": \".product-price\"\n",
        "})\n",
        "\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/ecommerce/\",\n",
        "    \"js_render\": True,\n",
        "    \"css_extractor\": css_selector\n",
        "})\n",
        "\n",
        "print(\"Extracted product data:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structured Data Extraction\n",
        "\n",
        "Automatically extract structured data like links, headings, tables, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract multiple data types automatically\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/ecommerce/\",\n",
        "    \"outputs\": \"links,headings\"\n",
        "})\n",
        "\n",
        "print(\"Extracted links and headings:\")\n",
        "print(result[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all tables from a webpage\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/table-parsing\",\n",
        "    \"outputs\": \"tables\"\n",
        "})\n",
        "\n",
        "print(\"Extracted tables:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Screenshots and Visual Capture\n",
        "\n",
        "Capture screenshots of entire pages or specific elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Capture full-page screenshot\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/ecommerce/\",\n",
        "    \"js_render\": True,\n",
        "    \"screenshot\": \"true\",\n",
        "    \"screenshot_fullpage\": \"true\"\n",
        "})\n",
        "\n",
        "# Save screenshot to file\n",
        "try:\n",
        "    with open(\"full_page_screenshot.png\", \"wb\") as f:\n",
        "        if isinstance(result, bytes):\n",
        "            f.write(result)\n",
        "        else:\n",
        "            f.write(base64.b64decode(result))\n",
        "    print(\"✅ Full-page screenshot saved as 'full_page_screenshot.png'\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Screenshot feature requires actual API key. Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Screenshot a specific element\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/ecommerce/\",\n",
        "    \"screenshot_selector\": \"#product-list\",\n",
        "    \"screenshot_format\": \"jpeg\",\n",
        "    \"screenshot_quality\": 85\n",
        "})\n",
        "\n",
        "# Save element screenshot\n",
        "try:\n",
        "    with open(\"products_grid_screenshot.jpg\", \"wb\") as f:\n",
        "        if isinstance(result, bytes):\n",
        "            f.write(result)\n",
        "        else:\n",
        "            f.write(base64.b64decode(result))\n",
        "    print(\"✅ Products grid screenshot saved as 'products_grid_screenshot.jpg'\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Screenshot feature requires actual API key. Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Premium Proxies and Geo-targeting\n",
        "\n",
        "Access geo-restricted content using premium proxies from different countries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check IP location with premium proxy from US\n",
        "result_us = scraper.invoke({\n",
        "    \"url\": \"https://httpbin.io/ip\",\n",
        "    \"premium_proxy\": True,\n",
        "    \"proxy_country\": \"us\"\n",
        "})\n",
        "\n",
        "print(\"Request from US IP:\")\n",
        "print(result_us)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with different country (UK)\n",
        "result_uk = scraper.invoke({\n",
        "    \"url\": \"https://httpbin.io/ip\",\n",
        "    \"premium_proxy\": True,\n",
        "    \"proxy_country\": \"gb\"  # Great Britain\n",
        "})\n",
        "\n",
        "print(\"Request from UK IP:\")\n",
        "print(result_uk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom JavaScript Execution\n",
        "\n",
        "Execute custom JavaScript to interact with page elements, fill forms, or click buttons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute custom JavaScript to interact with elements\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/login\",\n",
        "    \"js_instructions\": \"\"\"[{\"fill\":[\"#email\",\"admin@example.com\"]},\n",
        "                        {\"fill\":[\"#password\",\"password\"]},\n",
        "                        {\"click\":\"#submit-button\"},\n",
        "                        {\"wait\":500}]\"\"\"\n",
        "})\n",
        "\n",
        "print(\"Data extracted after JavaScript interactions:\")\n",
        "print(result[:300] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Session Management\n",
        "\n",
        "Maintain consistent sessions across multiple requests for multi-step processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "session_id = 12345  # Use same session for related requests\n",
        "\n",
        "# Step 1: Login or initial page\n",
        "result1 = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/login\",\n",
        "    \"premium_proxy\": True,\n",
        "    \"session_id\": session_id,\n",
        "    \"js_instructions\": \"\"\"[{\"fill\":[\"#email\",\"admin@example.com\"]},\n",
        "                        {\"fill\":[\"#password\",\"password\"]},\n",
        "                        {\"click\":\"#submit-button\"},\n",
        "                        {\"wait\":500}]\"\"\"\n",
        "})\n",
        "\n",
        "print(\"First request (login page):\")\n",
        "print(result1[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Access protected content with same session\n",
        "result2 = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/dashboard\",\n",
        "    \"premium_proxy\": True,\n",
        "    \"session_id\": session_id\n",
        "})\n",
        "\n",
        "print(\"Second request (dashboard with same session):\")\n",
        "print(result2[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JSON API Capture\n",
        "\n",
        "Capture JSON API calls made by web pages to access underlying data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Capture network requests and API calls\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/javascript-rendering\",\n",
        "    \"json_response\": True,\n",
        "    \"wait\": 3000  # Wait for API calls to complete\n",
        "})\n",
        "\n",
        "print(\"Captured JSON API responses:\")\n",
        "print(result[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Headers\n",
        "\n",
        "Use custom headers to mimic specific browser behavior or bypass certain restrictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scrape with custom headers\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://httpbin.io/headers\",\n",
        "    \"js_render\": True,\n",
        "    \"custom_headers\": {\"Referer\": \"https://google.com\"}\n",
        "})\n",
        "\n",
        "print(\"Response with custom headers:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using with LangChain Agents\n",
        "\n",
        "This is where the real power comes in - using ZenRows with LangChain agents for intelligent web scraping.\n",
        "\n",
        "**Note:** Make sure you have the required dependencies installed:\n",
        "```bash\n",
        "pip install langchain-openai langgraph\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from langgraph.prebuilt import create_react_agent\n",
        "    \n",
        "    # Initialize components\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    zenrows_tool = ZenRowsUniversalScraper()\n",
        "    \n",
        "    # Create agent\n",
        "    agent = create_react_agent(llm, [zenrows_tool])\n",
        "    \n",
        "    print(\"✅ Agent created successfully!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"❌ Missing dependencies: {e}\")\n",
        "    print(\"Please install: pip install langchain-openai langgraph\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error creating agent: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the agent to scrape and analyze Hacker News\n",
        "try:\n",
        "    result = agent.invoke({\n",
        "        \"messages\": \"Scrape https://news.ycombinator.com/ and list the top 3 stories with title, points, comments, username, and time.\"\n",
        "    })\n",
        "    \n",
        "    print(\"Agent Response:\")\n",
        "    for message in result[\"messages\"]:\n",
        "        print(f\"{message.content}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "except NameError:\n",
        "    print(\"⚠️  Agent not available - please run the previous cell successfully first\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error running agent: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced agent example: News summarizer\n",
        "try:\n",
        "    result = agent.invoke({\n",
        "        \"messages\": \"Go to TechCrunch.com, scrape the homepage in markdown format, and provide a summary of the top 5 technology stories with their headlines and brief descriptions.\"\n",
        "    })\n",
        "    \n",
        "    print(\"Tech News Summary:\")\n",
        "    for message in result[\"messages\"]:\n",
        "        print(f\"{message.content}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "except NameError:\n",
        "    print(\"⚠️  Agent not available - please run the agent creation cell first\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error running agent: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling and Best Practices\n",
        "\n",
        "The tool provides comprehensive error handling for various scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test error handling with invalid API key\n",
        "try:\n",
        "    invalid_scraper = ZenRowsUniversalScraper(zenrows_api_key=\"invalid-key\")\n",
        "    result = invalid_scraper.invoke({\"url\": \"https://httpbin.io/html\"})\n",
        "except ValueError as e:\n",
        "    if \"Invalid ZenRows API key\" in str(e):\n",
        "        print(\"✅ Correctly caught invalid API key error\")\n",
        "        print(f\"Error message: {e}\")\n",
        "    elif \"Rate limit exceeded\" in str(e):\n",
        "        print(\"⚠️  Rate limit exceeded - please upgrade your ZenRows plan\")\n",
        "    elif \"Response size too large\" in str(e):\n",
        "        print(\"⚠️  Response too large - use CSS selectors to reduce content\")\n",
        "    else:\n",
        "        print(f\"❌ Unexpected error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test error handling with invalid URL\n",
        "try:\n",
        "    result = scraper.invoke({\"url\": \"not-a-valid-url\"})\n",
        "except ValueError as e:\n",
        "    print(\"✅ Correctly caught invalid URL error\")\n",
        "    print(f\"Error message: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error type: {type(e).__name__}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Tips and Advanced Configuration\n",
        "\n",
        "Optimize your scraping performance with these advanced techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Block unnecessary resources to speed up scraping\n",
        "result = scraper.invoke({\n",
        "    \"url\": \"https://www.scrapingcourse.com/ecommerce/\",\n",
        "    \"js_render\": True,\n",
        "    \"block_resources\": \"images,fonts,media\",  # Block images, fonts, and media\n",
        "    \"wait_for\": \".product-name\",\n",
        "    \"response_type\": \"markdown\"\n",
        "})\n",
        "\n",
        "print(\"Fast scraping with blocked resources:\")\n",
        "print(result[:400] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook has demonstrated the comprehensive features of the `langchain-zenrows` package:\n",
        "\n",
        "### Core Features Covered:\n",
        "1. **Basic web scraping** with multiple output formats (HTML, Markdown, Plaintext)\n",
        "2. **JavaScript rendering** for modern SPAs and dynamic content\n",
        "3. **CSS extraction** for targeted data retrieval\n",
        "4. **Structured data extraction** (links, headings, tables, emails, etc.)\n",
        "5. **Screenshots** - full page and element-specific\n",
        "6. **Premium proxies** with geo-targeting (190+ countries)\n",
        "7. **Custom JavaScript execution** for complex interactions\n",
        "8. **Session management** for multi-step processes\n",
        "9. **JSON API capture** for intercepting network requests\n",
        "10. **Custom headers** for advanced request customization\n",
        "11. **LangChain agent integration** for intelligent scraping workflows\n",
        "12. **Error handling** and performance optimization\n",
        "\n",
        "### Key Benefits:\n",
        "- **55M+ residential IPs** for bypassing anti-bot systems\n",
        "- **JavaScript rendering** with headless browsers\n",
        "- **Multiple output formats** for different use cases\n",
        "- **Structured data extraction** without writing custom parsers\n",
        "- **Agent integration** for AI-powered scraping workflows\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the [official ZenRows API documentation](https://docs.zenrows.com/universal-scraper-api/api-reference#parameter-overview) for all available parameters\n",
        "- Check out the [LangChain documentation](https://python.langchain.com/) for more agent patterns\n",
        "- Build custom scraping workflows combining multiple features\n",
        "- Integrate with your existing LangChain applications\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [ZenRows Documentation](https://docs.zenrows.com/)\n",
        "- [LangChain Documentation](https://python.langchain.com/)\n",
        "- [Package Repository](https://github.com/ZenRows-Hub/langchain-zenrows)\n",
        "- [ZenRows Universal Scraper](https://app.zenrows.com/register?prod=universal_scraper)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
